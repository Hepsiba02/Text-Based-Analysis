{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c9a7efa",
   "metadata": {},
   "source": [
    "**FOR ENGLISH TEXT-1:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36aace28",
   "metadata": {},
   "source": [
    "**IMPORTING THE MODULES:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8e85a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords #you can remove stop words for Speed\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import numpy as np\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42386c75",
   "metadata": {},
   "source": [
    "**SPLITTING INTO SENTENCES:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10a3cd67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatGPT, an AI designed to assist with a wide range of tasks\n",
      "My primary function is to understand and generate human-like text based on the prompts I receive\n",
      "I've been trained on a diverse dataset that includes books, articles, websites, and much more, allowing me to provide information, answer questions, generate creative content, and engage in conversations on a variety of topics\n",
      "Essentially, I'm here to help you with whatever you need.\n"
     ]
    }
   ],
   "source": [
    "file = open(\"TEXT-1.txt\", \"r\")\n",
    "#This file contains one paragraph of multiple sentences\n",
    "filedata = file.readlines()\n",
    "article = filedata[0].split(\". \") #Just do the first paragraph\n",
    "sentences = []\n",
    "for sentence in article:\n",
    "    print(sentence)\n",
    "    sentences.append(sentence.replace(\"[^a-zA-Z]\", \" \").split(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e496a65",
   "metadata": {},
   "source": [
    "**PRINTING THE SENTENCES:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce8363e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences are  [['ChatGPT,', 'an', 'AI', 'designed', 'to', 'assist', 'with', 'a', 'wide', 'range', 'of', 'tasks'], ['My', 'primary', 'function', 'is', 'to', 'understand', 'and', 'generate', 'human-like', 'text', 'based', 'on', 'the', 'prompts', 'I', 'receive'], [\"I've\", 'been', 'trained', 'on', 'a', 'diverse', 'dataset', 'that', 'includes', 'books,', 'articles,', 'websites,', 'and', 'much', 'more,', 'allowing', 'me', 'to', 'provide', 'information,', 'answer', 'questions,', 'generate', 'creative', 'content,', 'and', 'engage', 'in', 'conversations', 'on', 'a', 'variety', 'of', 'topics'], ['Essentially,', \"I'm\", 'here', 'to', 'help', 'you', 'with', 'whatever', 'you', 'need.']]\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentences are \", sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c45f98",
   "metadata": {},
   "source": [
    "**BUILDING THE VECTOR FOR SENT1&2:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8062ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_similarity(sent1, sent2 ):\n",
    "    sent1 = [w.lower() for w in sent1]\n",
    "    sent2 = [w.lower() for w in sent2]\n",
    "    all_words = list(set(sent1 + sent2))\n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    "   # build the vector for the first sentence\n",
    "    for w in sent1:\n",
    "        vector1[all_words.index(w)] += 1\n",
    "    # build the vector for the second sentence\n",
    "    for w in sent2:\n",
    "        vector2[all_words.index(w)] += 1\n",
    "    return 1 - cosine_distance(vector1, vector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727c651a",
   "metadata": {},
   "source": [
    "**SIMILARITY MATRIX:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b7fdf25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smilarity matrix \n",
      " [[0.         0.07216878 0.18257419 0.16666667]\n",
      " [0.07216878 0.         0.23717082 0.07216878]\n",
      " [0.18257419 0.23717082 0.         0.04564355]\n",
      " [0.16666667 0.07216878 0.04564355 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    "\n",
    "for idx1 in range(len(sentences)):\n",
    "        for idx2 in range(len(sentences)):\n",
    "             if idx1 == idx2: #ignore if both are same sentences\n",
    "                continue\n",
    "             similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1],sentences[idx2])\n",
    "print(\"Smilarity matrix \\n\", similarity_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fcd996",
   "metadata": {},
   "source": [
    "**SCORES IN SIMILARITY MATRIX:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38b71680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores {0: 0.27016891494337747, 1: 0.2450355180000465, 2: 0.29267383593004126, 3: 0.1921217311265349}\n"
     ]
    }
   ],
   "source": [
    "# Step 3 - Rank sentences in similarity martix\n",
    "sentence_similarity_graph =nx.from_numpy_array(similarity_matrix)\n",
    "scores = nx.pagerank(sentence_similarity_graph)\n",
    "print(\"scores\", scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ca198b",
   "metadata": {},
   "source": [
    "**TOP RANKED SENTENCES:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "082de049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexes of top ranked_sentence order are \n",
      "\n",
      " [(0.29267383593004126, [\"I've\", 'been', 'trained', 'on', 'a', 'diverse', 'dataset', 'that', 'includes', 'books,', 'articles,', 'websites,', 'and', 'much', 'more,', 'allowing', 'me', 'to', 'provide', 'information,', 'answer', 'questions,', 'generate', 'creative', 'content,', 'and', 'engage', 'in', 'conversations', 'on', 'a', 'variety', 'of', 'topics']), (0.27016891494337747, ['ChatGPT,', 'an', 'AI', 'designed', 'to', 'assist', 'with', 'a', 'wide', 'range', 'of', 'tasks']), (0.2450355180000465, ['My', 'primary', 'function', 'is', 'to', 'understand', 'and', 'generate', 'human-like', 'text', 'based', 'on', 'the', 'prompts', 'I', 'receive']), (0.1921217311265349, ['Essentially,', \"I'm\", 'here', 'to', 'help', 'you', 'with', 'whatever', 'you', 'need.'])]\n"
     ]
    }
   ],
   "source": [
    "# Step 4 - Sort the rank and pick top sentences\n",
    "ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
    "print(\"Indexes of top ranked_sentence order are \\n\\n\",ranked_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df66b812",
   "metadata": {},
   "source": [
    "**PICKING THREE SENTENCES:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "499486b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many sentences do you want in the summary? 3\n"
     ]
    }
   ],
   "source": [
    "#Step 5 - How many sentences to pick\n",
    "n = int(input(\"How many sentences do you want in the summary? \"))\n",
    "#n=3\n",
    "summarize_text = []\n",
    "for i in range(n):\n",
    "    summarize_text.append(\" \".join(ranked_sentence[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e151b8",
   "metadata": {},
   "source": [
    "**SUMMERIZING THE TEXT:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3965172d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize Text: \n",
      " I've been trained on a diverse dataset that includes books, articles, websites, and much more, allowing me to provide information, answer questions, generate creative content, and engage in conversations on a variety of topics. ChatGPT, an AI designed to assist with a wide range of tasks. My primary function is to understand and generate human-like text based on the prompts I receive\n"
     ]
    }
   ],
   "source": [
    "# Step 6 - Offcourse, output the summarize text\n",
    "print(\"Summarize Text: \\n\", \". \".join(summarize_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618143d8",
   "metadata": {},
   "source": [
    "**FOR TELUGU TEXT-2:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fbe66a",
   "metadata": {},
   "source": [
    "**IMPORTING THE MODULES:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad1925f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords #you can remove stop words for Speed\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import numpy as np\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf555ab",
   "metadata": {},
   "source": [
    "**SPLITTING INTO SENTENCES:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3079a24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatGPT, విస్తృత శ్రేణి పనులలో సహాయం చేయడానికి రూపొందించబడిన AI\n",
      "నేను స్వీకరించే ప్రాంప్ట్‌ల ఆధారంగా మనిషిని పోలిన వచనాన్ని అర్థం చేసుకోవడం మరియు రూపొందించడం నా ప్రాథమిక విధి\n",
      "పుస్తకాలు, కథనాలు, వెబ్‌సైట్‌లు మరియు మరిన్నింటిని కలిగి ఉన్న విభిన్న డేటాసెట్‌పై నేను శిక్షణ పొందాను, సమాచారాన్ని అందించడానికి, ప్రశ్నలకు సమాధానం ఇవ్వడానికి, సృజనాత్మక కంటెంట్‌ను రూపొందించడానికి మరియు విభిన్న అంశాలపై సంభాషణలలో పాల్గొనడానికి నన్ను అనుమతిస్తుంది\n",
      "ముఖ్యంగా, మీకు అవసరమైన వాటితో మీకు సహాయం చేయడానికి నేను ఇక్కడ ఉన్నాను.\n"
     ]
    }
   ],
   "source": [
    "file = open(\"TEXT-2.txt\", \"r\")\n",
    "#This file contains one paragraph of multiple sentences\n",
    "filedata = file.readlines()\n",
    "article = filedata[0].split(\". \") #Just do the first paragraph\n",
    "sentences = []\n",
    "for sentence in article:\n",
    "    print(sentence)\n",
    "    sentences.append(sentence.replace(\"[^a-zA-Z]\", \" \").split(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7492dc36",
   "metadata": {},
   "source": [
    "**PRINTING THE SENTENCES:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2722794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences are  [['ChatGPT,', 'విస్తృత', 'శ్రేణి', 'పనులలో', 'సహాయం', 'చేయడానికి', 'రూపొందించబడిన', 'AI'], ['నేను', 'స్వీకరించే', 'ప్రాంప్ట్\\u200cల', 'ఆధారంగా', 'మనిషిని', 'పోలిన', 'వచనాన్ని', 'అర్థం', 'చేసుకోవడం', 'మరియు', 'రూపొందించడం', 'నా', 'ప్రాథమిక', 'విధి'], ['పుస్తకాలు,', 'కథనాలు,', 'వెబ్\\u200cసైట్\\u200cలు', 'మరియు', 'మరిన్నింటిని', 'కలిగి', 'ఉన్న', 'విభిన్న', 'డేటాసెట్\\u200cపై', 'నేను', 'శిక్షణ', 'పొందాను,', 'సమాచారాన్ని', 'అందించడానికి,', 'ప్రశ్నలకు', 'సమాధానం', 'ఇవ్వడానికి,', 'సృజనాత్మక', 'కంటెంట్\\u200cను', 'రూపొందించడానికి', 'మరియు', 'విభిన్న', 'అంశాలపై', 'సంభాషణలలో', 'పాల్గొనడానికి', 'నన్ను', 'అనుమతిస్తుంది'], ['ముఖ్యంగా,', 'మీకు', 'అవసరమైన', 'వాటితో', 'మీకు', 'సహాయం', 'చేయడానికి', 'నేను', 'ఇక్కడ', 'ఉన్నాను.']]\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentences are \", sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6d0ab0",
   "metadata": {},
   "source": [
    "**BUILDING THE VECTOR FOR SENT1&2:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2159280c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_similarity(sent1, sent2 ):\n",
    "    sent1 = [w.lower() for w in sent1]\n",
    "    sent2 = [w.lower() for w in sent2]\n",
    "    all_words = list(set(sent1 + sent2))\n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    "   # build the vector for the first sentence\n",
    "    for w in sent1:\n",
    "        vector1[all_words.index(w)] += 1\n",
    "    # build the vector for the second sentence\n",
    "    for w in sent2:\n",
    "        vector2[all_words.index(w)] += 1\n",
    "    return 1 - cosine_distance(vector1, vector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4593c5",
   "metadata": {},
   "source": [
    "**SIMILARITY MATRIX:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bdcb19f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smilarity matrix \n",
      " [[0.         0.         0.         0.20412415]\n",
      " [0.         0.         0.14400461 0.07715167]\n",
      " [0.         0.14400461 0.         0.05184758]\n",
      " [0.20412415 0.07715167 0.05184758 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    "\n",
    "for idx1 in range(len(sentences)):\n",
    "        for idx2 in range(len(sentences)):\n",
    "             if idx1 == idx2: #ignore if both are same sentences\n",
    "                continue\n",
    "             similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1],sentences[idx2])\n",
    "print(\"Smilarity matrix \\n\", similarity_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9383a0d0",
   "metadata": {},
   "source": [
    "**SCORES IN SIMILARITY MATRIX:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "992238e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores {0: 0.21289610177103374, 1: 0.23708278321290502, 2: 0.2132691888774271, 3: 0.33675192613863414}\n"
     ]
    }
   ],
   "source": [
    "# Step 3 - Rank sentences in similarity martix\n",
    "sentence_similarity_graph =nx.from_numpy_array(similarity_matrix)\n",
    "scores = nx.pagerank(sentence_similarity_graph)\n",
    "print(\"scores\", scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e0cc93",
   "metadata": {},
   "source": [
    "**TOP RANKED SENTENCES:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff8362ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexes of top ranked_sentence order are \n",
      "\n",
      " [(0.33675192613863414, ['ముఖ్యంగా,', 'మీకు', 'అవసరమైన', 'వాటితో', 'మీకు', 'సహాయం', 'చేయడానికి', 'నేను', 'ఇక్కడ', 'ఉన్నాను.']), (0.23708278321290502, ['నేను', 'స్వీకరించే', 'ప్రాంప్ట్\\u200cల', 'ఆధారంగా', 'మనిషిని', 'పోలిన', 'వచనాన్ని', 'అర్థం', 'చేసుకోవడం', 'మరియు', 'రూపొందించడం', 'నా', 'ప్రాథమిక', 'విధి']), (0.2132691888774271, ['పుస్తకాలు,', 'కథనాలు,', 'వెబ్\\u200cసైట్\\u200cలు', 'మరియు', 'మరిన్నింటిని', 'కలిగి', 'ఉన్న', 'విభిన్న', 'డేటాసెట్\\u200cపై', 'నేను', 'శిక్షణ', 'పొందాను,', 'సమాచారాన్ని', 'అందించడానికి,', 'ప్రశ్నలకు', 'సమాధానం', 'ఇవ్వడానికి,', 'సృజనాత్మక', 'కంటెంట్\\u200cను', 'రూపొందించడానికి', 'మరియు', 'విభిన్న', 'అంశాలపై', 'సంభాషణలలో', 'పాల్గొనడానికి', 'నన్ను', 'అనుమతిస్తుంది']), (0.21289610177103374, ['ChatGPT,', 'విస్తృత', 'శ్రేణి', 'పనులలో', 'సహాయం', 'చేయడానికి', 'రూపొందించబడిన', 'AI'])]\n"
     ]
    }
   ],
   "source": [
    "# Step 4 - Sort the rank and pick top sentences\n",
    "ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
    "print(\"Indexes of top ranked_sentence order are \\n\\n\",ranked_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3031ae0",
   "metadata": {},
   "source": [
    "**PICKING THREE SENTENCES:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e77322e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many sentences do you want in the summary? 3\n"
     ]
    }
   ],
   "source": [
    "#Step 5 - How many sentences to pick\n",
    "n = int(input(\"How many sentences do you want in the summary? \"))\n",
    "#n=3\n",
    "summarize_text = []\n",
    "for i in range(n):\n",
    "    summarize_text.append(\" \".join(ranked_sentence[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6669559d",
   "metadata": {},
   "source": [
    "**SUMMERIZING THE TEXT:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f39e764d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize Text: \n",
      " ముఖ్యంగా, మీకు అవసరమైన వాటితో మీకు సహాయం చేయడానికి నేను ఇక్కడ ఉన్నాను.. నేను స్వీకరించే ప్రాంప్ట్‌ల ఆధారంగా మనిషిని పోలిన వచనాన్ని అర్థం చేసుకోవడం మరియు రూపొందించడం నా ప్రాథమిక విధి. పుస్తకాలు, కథనాలు, వెబ్‌సైట్‌లు మరియు మరిన్నింటిని కలిగి ఉన్న విభిన్న డేటాసెట్‌పై నేను శిక్షణ పొందాను, సమాచారాన్ని అందించడానికి, ప్రశ్నలకు సమాధానం ఇవ్వడానికి, సృజనాత్మక కంటెంట్‌ను రూపొందించడానికి మరియు విభిన్న అంశాలపై సంభాషణలలో పాల్గొనడానికి నన్ను అనుమతిస్తుంది\n"
     ]
    }
   ],
   "source": [
    "# Step 6 - Offcourse, output the summarize text\n",
    "print(\"Summarize Text: \\n\", \". \".join(summarize_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e74c6b",
   "metadata": {},
   "source": [
    "**FOR HINDI TEXT-3:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25e848d",
   "metadata": {},
   "source": [
    "**IMPORTING THE MODULES:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6767efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords #you can remove stop words for Speed\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import numpy as np\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef235b4",
   "metadata": {},
   "source": [
    "**SPLITTING INTO SENTENCES:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d216298b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "चैटजीपीटी, एक एआई जिसे कई प्रकार के कार्यों में सहायता के लिए डिज़ाइन किया गया है। मेरा प्राथमिक कार्य मुझे प्राप्त संकेतों के आधार पर मानव-सदृश पाठ को समझना और उत्पन्न करना है। मुझे एक विविध डेटासेट पर प्रशिक्षित किया गया है जिसमें किताबें, लेख, वेबसाइटें और बहुत कुछ शामिल है, जो मुझे जानकारी प्रदान करने, सवालों के जवाब देने, रचनात्मक सामग्री तैयार करने और विभिन्न विषयों पर बातचीत में शामिल होने की अनुमति देता है। मूलतः, मैं आपकी हर जरूरत में मदद करने के लिए यहां हूं।\n"
     ]
    }
   ],
   "source": [
    "file = open(\"TEXT-3.txt\", \"r\")\n",
    "#This file contains one paragraph of multiple sentences\n",
    "filedata = file.readlines()\n",
    "article = filedata[0].split(\". \") #Just do the first paragraph\n",
    "sentences = []\n",
    "for sentence in article:\n",
    "    print(sentence)\n",
    "    sentences.append(sentence.replace(\"[^a-zA-Z]\", \" \").split(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3204b0d3",
   "metadata": {},
   "source": [
    "**PRINTING THE SENTENCES:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b5dc0391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences are  [['चैटजीपीटी,', 'एक', 'एआई', 'जिसे', 'कई', 'प्रकार', 'के', 'कार्यों', 'में', 'सहायता', 'के', 'लिए', 'डिज़ाइन', 'किया', 'गया', 'है।', 'मेरा', 'प्राथमिक', 'कार्य', 'मुझे', 'प्राप्त', 'संकेतों', 'के', 'आधार', 'पर', 'मानव-सदृश', 'पाठ', 'को', 'समझना', 'और', 'उत्पन्न', 'करना', 'है।', 'मुझे', 'एक', 'विविध', 'डेटासेट', 'पर', 'प्रशिक्षित', 'किया', 'गया', 'है', 'जिसमें', 'किताबें,', 'लेख,', 'वेबसाइटें', 'और', 'बहुत', 'कुछ', 'शामिल', 'है,', 'जो', 'मुझे', 'जानकारी', 'प्रदान', 'करने,', 'सवालों', 'के', 'जवाब', 'देने,', 'रचनात्मक', 'सामग्री', 'तैयार', 'करने', 'और', 'विभिन्न', 'विषयों', 'पर', 'बातचीत', 'में', 'शामिल', 'होने', 'की', 'अनुमति', 'देता', 'है।', 'मूलतः,', 'मैं', 'आपकी', 'हर', 'जरूरत', 'में', 'मदद', 'करने', 'के', 'लिए', 'यहां', 'हूं।']]\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentences are \", sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ded1a0",
   "metadata": {},
   "source": [
    "**BUILDING THE VECTOR OF SENT1&2:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d488457d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_similarity(sent1, sent2 ):\n",
    "    sent1 = [w.lower() for w in sent1]\n",
    "    sent2 = [w.lower() for w in sent2]\n",
    "    all_words = list(set(sent1 + sent2))\n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    "   # build the vector for the first sentence\n",
    "    for w in sent1:\n",
    "        vector1[all_words.index(w)] += 1\n",
    "    # build the vector for the second sentence\n",
    "    for w in sent2:\n",
    "        vector2[all_words.index(w)] += 1\n",
    "    return 1 - cosine_distance(vector1, vector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7438b102",
   "metadata": {},
   "source": [
    "**SIMILARITY MATRIX:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "54536ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smilarity matrix \n",
      " [[0.]]\n"
     ]
    }
   ],
   "source": [
    "similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    "\n",
    "for idx1 in range(len(sentences)):\n",
    "        for idx2 in range(len(sentences)):\n",
    "             if idx1 == idx2: #ignore if both are same sentences\n",
    "                continue\n",
    "             similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1],sentences[idx2])\n",
    "print(\"Smilarity matrix \\n\", similarity_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2de106",
   "metadata": {},
   "source": [
    "**SCORES IN SIMILARITY MATRIX:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0586d18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores {0: 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Step 3 - Rank sentences in similarity martix\n",
    "sentence_similarity_graph =nx.from_numpy_array(similarity_matrix)\n",
    "scores = nx.pagerank(sentence_similarity_graph)\n",
    "print(\"scores\", scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610c8051",
   "metadata": {},
   "source": [
    "**TOP RANKED SENTENCES:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "98dc96f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexes of top ranked_sentence order are \n",
      "\n",
      " [(1.0, ['चैटजीपीटी,', 'एक', 'एआई', 'जिसे', 'कई', 'प्रकार', 'के', 'कार्यों', 'में', 'सहायता', 'के', 'लिए', 'डिज़ाइन', 'किया', 'गया', 'है।', 'मेरा', 'प्राथमिक', 'कार्य', 'मुझे', 'प्राप्त', 'संकेतों', 'के', 'आधार', 'पर', 'मानव-सदृश', 'पाठ', 'को', 'समझना', 'और', 'उत्पन्न', 'करना', 'है।', 'मुझे', 'एक', 'विविध', 'डेटासेट', 'पर', 'प्रशिक्षित', 'किया', 'गया', 'है', 'जिसमें', 'किताबें,', 'लेख,', 'वेबसाइटें', 'और', 'बहुत', 'कुछ', 'शामिल', 'है,', 'जो', 'मुझे', 'जानकारी', 'प्रदान', 'करने,', 'सवालों', 'के', 'जवाब', 'देने,', 'रचनात्मक', 'सामग्री', 'तैयार', 'करने', 'और', 'विभिन्न', 'विषयों', 'पर', 'बातचीत', 'में', 'शामिल', 'होने', 'की', 'अनुमति', 'देता', 'है।', 'मूलतः,', 'मैं', 'आपकी', 'हर', 'जरूरत', 'में', 'मदद', 'करने', 'के', 'लिए', 'यहां', 'हूं।'])]\n"
     ]
    }
   ],
   "source": [
    "# Step 4 - Sort the rank and pick top sentences\n",
    "ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
    "print(\"Indexes of top ranked_sentence order are \\n\\n\",ranked_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b916de71",
   "metadata": {},
   "source": [
    "**PICKKING ONE SENTENCE:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c7c713c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many sentences do you want in the summary? 1\n"
     ]
    }
   ],
   "source": [
    "#Step 5 - How many sentences to pick\n",
    "n = int(input(\"How many sentences do you want in the summary? \"))\n",
    "#n=1\n",
    "summarize_text = []\n",
    "for i in range(n):\n",
    "    summarize_text.append(\" \".join(ranked_sentence[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509f3533",
   "metadata": {},
   "source": [
    "**SUMMERIZING THE TEXT:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eaf90e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize Text: \n",
      " चैटजीपीटी, एक एआई जिसे कई प्रकार के कार्यों में सहायता के लिए डिज़ाइन किया गया है। मेरा प्राथमिक कार्य मुझे प्राप्त संकेतों के आधार पर मानव-सदृश पाठ को समझना और उत्पन्न करना है। मुझे एक विविध डेटासेट पर प्रशिक्षित किया गया है जिसमें किताबें, लेख, वेबसाइटें और बहुत कुछ शामिल है, जो मुझे जानकारी प्रदान करने, सवालों के जवाब देने, रचनात्मक सामग्री तैयार करने और विभिन्न विषयों पर बातचीत में शामिल होने की अनुमति देता है। मूलतः, मैं आपकी हर जरूरत में मदद करने के लिए यहां हूं।\n"
     ]
    }
   ],
   "source": [
    "# Step 6 - Offcourse, output the summarize text\n",
    "print(\"Summarize Text: \\n\", \". \".join(summarize_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab249a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
